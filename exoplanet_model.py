# -*- coding: utf-8 -*-
"""Reduced Exoplanet Habitability Prediction

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/18vyKNHlSXehV2Mq6knEocdF0EZ16XpU3
"""

# Importing Data from drive
import gdown
url = 'https://drive.google.com/uc?id=1rueUc3n7KipFd7CEXrYseZUza2c71dSK'
output = 'exoplanetdata.csv'
gdown.download(url, output,quiet=False)

import pandas as pd
import numpy as np
import seaborn as sns

dataset = pd.read_csv('exoplanetdata.csv')
dataset.info()

print(dataset.isnull().sum())

#Visulazing dataset
sns.heatmap(dataset.isnull(),cmap='Accent_r')

# Removing attributes with more than 40% missing data
count = dataset.isnull().sum().sort_values(ascending=False)
percent = ((dataset.isnull().sum()/dataset.isnull().count())*100).sort_values(ascending=False)
missing = pd.concat([count,percent],axis=1,keys=['count','%'])
missing = missing[lambda x :missing['%']>60]
missing.head(60)

dataset_cols_dropped = dataset.drop(['P_DETECTION_MASS', 'P_GEO_ALBEDO',
'S_MAGNETIC_FIELD', 'S_DISC', 'P_ATMOSPHERE', 'P_ALT_NAMES',
'P_DETECTION_RADIUS', 'P_GEO_ALBEDO_ERROR_MIN', 'P_TEMP_MEASURED',
'P_GEO_ALBEDO_ERROR_MAX', 'P_TPERI_ERROR_MAX', 'P_TPERI_ERROR_MIN',
'P_TPERI', 'P_OMEGA_ERROR_MIN', 'P_OMEGA_ERROR_MAX', 'P_DENSITY',
'P_ESCAPE', 'P_POTENTIAL', 'P_GRAVITY', 'P_OMEGA',
'P_INCLINATION_ERROR_MAX', 'P_INCLINATION_ERROR_MIN', 'P_INCLINATION',
'P_ECCENTRICITY_ERROR_MAX', 'P_ECCENTRICITY_ERROR_MIN', 'S_TYPE',
'P_ECCENTRICITY','P_IMPACT_PARAMETER_ERROR_MIN',
'P_IMPACT_PARAMETER_ERROR_MAX', 'P_IMPACT_PARAMETER', 'P_MASS_ERROR_MAX','P_MASS_ERROR_MIN', 'P_HILL_SPHERE', 'P_SEMI_MAJOR_AXIS_ERROR_MIN',\
'P_SEMI_MAJOR_AXIS_ERROR_MAX', 'P_MASS', 'S_AGE_ERROR_MAX',
'S_AGE_ERROR_MIN', 'S_AGE', 'P_ANGULAR_DISTANCE', 'P_SEMI_MAJOR_AXIS'],

axis = 1)
dataset_cols_dropped.info()

dataset_cols_dropped.select_dtypes(include=['object']).columns
object_cols = dataset_cols_dropped.select_dtypes(include=['object'])
count = object_cols.isnull().sum().sort_values(ascending=False)
percent = ((object_cols.isnull().sum()/object_cols.isnull().count())*100).sort_values(ascending=False)
missing = pd.concat([count,percent],axis=1,keys=['Count','%'])
missing.head(13)

dataset_cols_dropped['P_TYPE_TEMP'] = dataset_cols_dropped['P_TYPE_TEMP'].fillna(dataset_cols_dropped['P_TYPE_TEMP'].mode()[0])
dataset_cols_dropped['S_TYPE_TEMP'] = dataset_cols_dropped['S_TYPE_TEMP'].fillna(dataset_cols_dropped['S_TYPE_TEMP'].mode()[0])
dataset_cols_dropped['P_TYPE'] = dataset_cols_dropped['P_TYPE'].fillna(dataset_cols_dropped['P_TYPE'].mode()[0])

from sklearn.preprocessing import LabelEncoder
encoders = {}
for col in dataset_cols_dropped.select_dtypes(include = ['object']).columns:
  encoders[col] = LabelEncoder()
  dataset_cols_dropped[col] = encoders[col].fit_transform(dataset_cols_dropped[col])

from sklearn.experimental import enable_iterative_imputer

from sklearn.impute import IterativeImputer
imputed_data = dataset_cols_dropped.copy(deep=True)
mice_imputer = IterativeImputer()
imputed_data.iloc[:, :] = mice_imputer.fit_transform(dataset_cols_dropped)
imputed_data.head(6)

imputed_data.isna().sum(axis=0)

from collections import Counter
counter_ = Counter(imputed_data['P_HABITABLE'])
for class_label_, example_num_ in counter_.items():
  percentage_ = example_num_ / len(imputed_data['P_HABITABLE'])*100
  print('class=%d, n=%d (%.3f%%)' % (class_label_, example_num_, percentage_))

from imblearn.combine import SMOTEENN
X, y = imputed_data.drop(['P_HABITABLE'], axis = 1), imputed_data.P_HABITABLE
smt = SMOTEENN(random_state=0)
X, y = smt.fit_resample(X,y)

from collections import Counter
counter = Counter(y)
for class_label, example_num in counter.items():
  percentage = example_num / len(y)*100
  print('Class=%d, n =%d (%.3f%%)' % (class_label, example_num, percentage))

sampled_data = X
sampled_data['P_HABITABLE'] = y
sampled_data.shape
correlation_mat = sampled_data.corr().abs()

import matplotlib.pyplot as plt
mask = np.triu(np.ones_like(correlation_mat, dtype = np.bool_))
f, ax = plt.subplots(figsize=(20,20))
cmap = sns.diverging_palette(255, 0 , as_cmap = True)
sns.heatmap(correlation_mat, mask = mask, cmap = cmap, vmax = None, center = 0, square = True, annot = False, linewidths = .5, cbar_kws = {"shrink":0.9})

upper_triangle = correlation_mat.where(np.triu(np.ones(correlation_mat.shape),k=1).astype(np.bool_))
to_drop = [column for column in upper_triangle.columns if any(upper_triangle[column]>.95)]
print(to_drop)
np.size(to_drop)

preprocessed_data = sampled_data.drop(['P_RADIUS_ERROR_MIN', 'P_RADIUS_ERROR_MAX', 'P_PERIOD_ERROR_MIN', 'P_PERIOD_ERROR_MAX', 'S_NAME', 'S_MAG', 'S_DISTANCE_ERROR_MIN', 'S_DISTANCE_ERROR_MAX', 'S_METALLICITY', 'S_METALLICITY_ERROR_MIN', 'S_METALLICITY_ERROR_MAX', 'S_RADIUS', 'S_TEMPERATURE_ERROR_MIN', 'S_TEMPERATURE_ERROR_MAX', 'S_LOG_G', 'P_PERIASTRON', 'P_APASTRON', 'P_DISTANCE_EFF', 'P_FLUX_MIN', 'P_FLUX_MAX', 'P_TEMP_EQUIL_MIN', 'P_TEMP_EQUIL_MAX', 'S_RADIUS_EST', 'S_RA_H', 'S_LUMINOSITY', 'S_HZ_OPT_MIN', 'S_HZ_OPT_MAX', 'S_HZ_CON_MIN', 'S_HZ_CON_MAX', 'S_HZ_CON0_MIN', 'S_HZ_CON0_MAX', 'S_HZ_CON1_MIN', 'S_HZ_CON1_MAX', 'S_SNOW_LINE', 'S_ABIO_ZONE', 'P_ESI', 'S_CONSTELLATION_ABR', 'P_SEMI_MAJOR_AXIS_EST']
, axis = 1)
preprocessed_data.head(10)
preprocessed_data.columns.values.tolist()

# relavance as per physics
# preprocessed_data.drop(['P_NAME',
#  'P_STATUS',

#  'P_YEAR',
#  'P_UPDATED',

#  'P_DETECTION',
#  'S_RA',
#  'S_DEC',

#  'S_MASS_ERROR_MIN',
#  'S_RADIUS_ERROR_MIN',
#  'S_RADIUS_ERROR_MAX',

#  'S_ALT_NAMES',

#  'P_FLUX',

#  'S_TYPE_TEMP',
#  'S_RA_T',
#  'S_DEC_T',

#  'P_HABZONE_OPT',
#  'P_HABZONE_CON',
#  'S_CONSTELLATION',
#  'S_CONSTELLATION_ENG',
#  'P_RADIUS_EST',

#  'P_HABITABLE'], axis = 1)

feature_mat = preprocessed_data.drop('P_HABITABLE', axis = 1)
target = preprocessed_data['P_HABITABLE']

from sklearn.feature_selection import SelectFromModel
from sklearn.ensemble import RandomForestClassifier as rf

estimator = rf(n_estimators = 1000, random_state = 0 )
selector = SelectFromModel(estimator)
selector.fit(feature_mat, target)

status = selector.get_support()
print("Status: ", status)

features = feature_mat.loc[:, status].columns.tolist()
print(features)

print(rf(n_estimators = 1000, random_state = 0).fit(feature_mat, target).feature_importances_)
# [['P_PERIOD', 'P_FLUX', 'P_TEMP_EQUIL','P_TYPE', 'P_HABZONE_OPT','P_RADIUS_EST', 'P_MASS_EST', 'S_TYPE_TEMP']]

from sklearn.ensemble import AdaBoostRegressor as Ada

estimator = Ada(random_state = 0, n_estimators = 50)
selector = SelectFromModel(estimator)
selector.fit(feature_mat, target)

status = selector.get_support()
print("Status: ", status)

featuers = feature_mat.loc[:, status].columns.tolist()
print(features)

print(estimator.fit(feature_mat, target).feature_importances_)

del features
del estimator
del selector
del status

from sklearn.ensemble import ExtraTreesClassifier as et

estimator = et(n_estimators = 1000, random_state = 0)
selector = SelectFromModel(estimator)
selector.fit(feature_mat, target)

status = selector.get_support()
print("Status: ", status)

features = feature_mat.loc[:, status].columns.tolist()
print(features)

print(estimator.fit(feature_mat, target).feature_importances_)

del feature_mat
del target
del X
del y

from sklearn.model_selection import train_test_split

feature_mat = preprocessed_data[['P_PERIOD', 'P_FLUX', 'P_TEMP_EQUIL',
'P_TYPE', 'P_HABZONE_OPT','P_RADIUS_EST', 'P_MASS_EST', 'S_TYPE_TEMP']]

target = preprocessed_data['P_HABITABLE']

X_train, X_test, y_train, y_test = train_test_split(feature_mat, target,
test_size = 0.33, random_state = 42)

print(f"X_train dimentions: {X_train.shape}")
print(f"X_test dimentions: {X_test.shape}")
print(f"y_train dimentions: {y_train.shape}")
print(f"y_test dimentions: {y_test.shape}")

feature_mat.info()

y = np.array(X_test)

y[0]

k = modelS.transform(y[0].reshape(1,8))

k

y_test.head(10)

from sklearn.preprocessing import MinMaxScaler

scaler = MinMaxScaler()
modelS = scaler.fit(X_train)
X_train = modelS.transform(X_train)
modelS2 = scaler.fit(X_test)
X_test = modelS2.transform(X_test)

from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import GridSearchCV

param_grid = {'max_depth': np.arange(2, 10, 1),
              'max_leaf_nodes': np.arange(2, 100, 10),
              'random_state': [0, 1, 2, 3, 4, 5],
              'splitter': ['best', 'random']}

grid_search = GridSearchCV(DecisionTreeClassifier(),
                           param_grid = param_grid,
                           refit = True, verbose = 0)

grid_search.fit(X_train, y_train)

print(f"Best Parameters: {grid_search.best_params_}")

from sklearn.metrics import confusion_matrix
def plot_confusion_mat(ytest, ypred):
  conf_mat=confusion_matrix(ytest, ypred)
  confusion_mat_df = pd.DataFrame(conf_mat,index=['Inhabitable', 'Consevatively Habitable','Optimistically Habitable'],
                         columns = ['Inhabitable', 'Consevatively Habitable','Optimistically Habitable'])
  plt.figure(figsize = (5,4))
  sns.heatmap(confusion_mat_df, annot = True)
  plt.title('Habitability Confusion Matrix')
  plt.ylabel('Actal Values')
  plt.xlabel('Predicted Values')
  plt.show()

  return conf_mat

import time
from sklearn.metrics import accuracy_score
from sklearn.metrics import classification_report

t0 = time.time()

dtree_model = DecisionTreeClassifier(splitter = 'random',
                                     max_depth = 6,
                                     random_state = 0,
                                     max_leaf_nodes = 12).fit(X_train, y_train)


dtree_predictions = dtree_model.predict(X_test)

dtree_time = time.time() - t0

dtree_accuracy = accuracy_score(y_test, dtree_predictions)

confusion_mat = plot_confusion_mat(y_test, dtree_predictions)


print(f"Decision Tree classification Report:\n")
print(classification_report(y_test, dtree_predictions))

del confusion_mat
del t0

from sklearn.neighbors import KNeighborsClassifier


t0 = time.time()

knn_model = KNeighborsClassifier(n_neighbors = 3).fit(X_train, y_train)


knn_predictions = knn_model.predict(X_test)

knn_time = time.time() - t0

knn_accuracy = accuracy_score(y_test, knn_predictions)

confusion_mat = plot_confusion_mat(y_test, knn_predictions)

print(f"KNN classification Report:\n")
print(classification_report(y_test, knn_predictions))

del confusion_mat
del t0

import warnings
warnings.simplefilter('ignore')
from sklearn.model_selection import GridSearchCV
from sklearn.ensemble import GradientBoostingClassifier

param_grid = {'criterion': ['friedman_mse',  'mse'],
              'n_estimators': np.arange(10, 100, 10)}

grid_search = GridSearchCV(GradientBoostingClassifier(),
                           param_grid = param_grid,
                           refit = True, verbose = 1)

grid_search.fit(X_train, y_train)

# print best parameter after tuning
print(f"Best Parameters: {grid_search.best_params_}")

from sklearn.metrics import classification_report

t0 = time.time()

gb_model = GradientBoostingClassifier(loss = 'deviance', n_estimators = 10,
                                criterion = 'friedman_mse', max_depth = 6,
                                random_state = 0, max_features = 'auto',
                                max_leaf_nodes = 12).fit(X_train, y_train)

#Test the model
gb_predictions = gb_model.predict(X_test)

#Calculate the time taken for further comparison
gb_time = time.time() - t0

#Store accuracy and elapsed time for final model comparison
gb_accuracy = accuracy_score(y_test, gb_predictions)


confusion_mat = plot_confusion_mat(y_test, gb_predictions)


print(f"Gradient Boosting classification Report:\n")
print(classification_report(y_test, gb_predictions))

del confusion_mat

def plot_model_log(log):

    fig, ax1 = plt.subplots(figsize = (10, 5))
    ax1.set_title('Accuracy and Time taken', fontsize = 13)
    color = 'tab:green'
    ax1.set_xlabel('Classifier', fontsize = 13)
    ax1.set_ylabel('Time taken', fontsize = 13, color = color)
    ax2 = sns.barplot(x = 'Classifier', y = 'Time taken', data = log, palette = 'summer')
    ax1.tick_params(axis = 'y')
    ax2 = ax1.twinx()
    color = 'tab:blue'
    ax2.set_ylabel('Accuracy', fontsize = 13, color = color)
    ax2 = sns.lineplot(x = 'Classifier', y = 'Accuracy', data = log, sort = False, color = color)
    ax2.tick_params(axis = 'y', color = color)

accuracy_list = [dtree_accuracy, knn_accuracy, gb_accuracy]

#Make a list of times
time_list = [dtree_time, knn_time, gb_time]

log_data = {'Classifier': ['Decision Tree','KNN','Gradient Boosting'],
              'Accuracy': accuracy_list,
              'Time taken': time_list}
clf_log = pd.DataFrame(log_data)

#Plot the log
plot_model_log(clf_log)
print(clf_log)

from joblib import dump, load
dump(modelS, open('modelS.joblib','wb'))
#clf2 = load(open('filename.joblib','rb'))

import pickle
pickle.dump(modelS,open('modelS.pkl','wb'))

X_test[4048]
X_test[6486 ]
X_test[6355]
X_test[10219]
X_test[6858]
X_test[8423]

y_test.head(10)

imputed_data['S_TYPE_TEMP'].unique()

dataset['S_TYPE_TEMP'].unique()

d

X_test[10]

x = 0
for y in y_test:
  x = x+1
  if (y ==2):
    print(X_test[x])

S = X_test
np.append(S, y_test, axis=1)

for x in preprocessed_data['P_PERIOD']:
    print(x)



